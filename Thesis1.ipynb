{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "EhtoySeQUBO5",
        "_Wlu-3ZIvCY9",
        "H50UP_9tw3jC",
        "KLRhefupwIbU"
      ],
      "authorship_tag": "ABX9TyNIFdetKqYA52BJfgGWJVy+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnbaohan/graduation_thesis/blob/Hybrid-Fourier-Domain-and-Deep-Learning-Approaches-for-Financial-Market-Forecasting-and-Pattern-Recognition/Thesis1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Setup"
      ],
      "metadata": {
        "id": "8qCwoZpcTqlq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ay60B_ZHTmsY"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install yfinance --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install scikeras --quiet\n",
        "!pip install tensorflow pandas numpy matplotlib --quiet\n",
        "!pip install TA-Lib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import math\n",
        "import pywt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fft import fft, ifft\n",
        "from datetime import datetime, timedelta\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Attention, Add, Dense, Input, MultiHeadAttention, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import talib  # Technical Analysis Library\n",
        "\n",
        "# Set plotting configuration\n",
        "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(0)\n",
        "\n",
        "# Create directories for results\n",
        "os.makedirs(\"results/models\", exist_ok=True)\n",
        "os.makedirs(\"results/plots\", exist_ok=True)"
      ],
      "metadata": {
        "id": "uLkok1-0Tw1Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "EhtoySeQUBO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the portfolio tickers and date range\n",
        "tickers = [\"VCB.VN\", \"CTG.VN\", \"BID.VN\", \"TCB.VN\", \"MBB.VN\"]\n",
        "start_date = \"2020-01-01\"\n",
        "end_date   = \"2025-01-01\""
      ],
      "metadata": {
        "id": "GB08gfi3uvs4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_yfinance_multiindex(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean yfinance multi-index columns and flatten them.\"\"\"\n",
        "    if data is None or data.shape[0] == 0:\n",
        "        return data  # return empty as-is\n",
        "\n",
        "    # Flatten multi-index columns if present\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = [col[0] if col[0] != \"\" else col[1] for col in data.columns]\n",
        "    # Remove any header rows that are ticker symbols\n",
        "    try:\n",
        "        if data.iloc[0].astype(str).str.fullmatch(r\"[A-Z]+\").any():\n",
        "            data = data.iloc[1:].reset_index(drop=True)\n",
        "    except IndexError:\n",
        "        return pd.DataFrame()  # return empty if indexing fails\n",
        "\n",
        "    # Standardize column names\n",
        "    rename_map = {\"Date\":\"Date\", \"Open\":\"Open\", \"High\":\"High\", \"Low\":\"Low\", \"Close\":\"Close\", \"Volume\":\"Volume\"}\n",
        "    data = data.rename(columns=rename_map)\n",
        "    # Ensure Date is datetime\n",
        "    if \"Date\" in data.columns:\n",
        "        data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "    # Convert price/volume columns to numeric\n",
        "    for col in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]:\n",
        "        if col in data.columns:\n",
        "            data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
        "    # Drop rows with NaN Close or Date\n",
        "    if \"Date\" in data.columns and \"Close\" in data.columns:\n",
        "        data = data.dropna(subset=[\"Date\",\"Close\"])\n",
        "    # Sort by date\n",
        "    if \"Date\" in data.columns:\n",
        "        data = data.sort_values(\"Date\").reset_index(drop=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "S0nPAaTYu3PW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(X, y, seq_len):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(seq_len, len(X)):\n",
        "        X_seq.append(X[i-seq_len:i])\n",
        "        y_seq.append(y[i])\n",
        "    return np.array(X_seq), np.array(y_seq)"
      ],
      "metadata": {
        "id": "DusAZbiFu7nF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "_Wlu-3ZIvCY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Container to collect evaluation metrics for all models and stocks\n",
        "all_metrics = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    # Download historical data for the stock\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    data = data.reset_index()               # Convert index to column for uniform processing\n",
        "    data = clean_yfinance_multiindex(data)  # Clean and standardize columns\n",
        "    print(f\"There are {data.shape[0]} days in the {ticker} dataset.\")\n",
        "\n",
        "    # Compute log returns and percentage change\n",
        "    data[\"Log_Return\"] = np.log(data[\"Close\"] / data[\"Close\"].shift(1))\n",
        "    data[\"Pct_Change\"] = data[\"Close\"].pct_change()\n",
        "    data = data.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJgpzwfTvAmI",
        "outputId": "4aa57f15-2654-417f-df38-cf7d9d8354fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1250 days in the VCB.VN dataset.\n",
            "There are 1246 days in the CTG.VN dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1250 days in the BID.VN dataset.\n",
            "There are 1247 days in the TCB.VN dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1250 days in the MBB.VN dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Model"
      ],
      "metadata": {
        "id": "H50UP_9tw3jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def evaluate_model(model, X_train_seq, y_train_seq, X_test_seq, y_test_seq, name=\"\", plot=True):\n",
        "        # Predict on train and test sets\n",
        "        y_pred_train = model.predict(X_train_seq, verbose=0).squeeze()\n",
        "        y_pred_test  = model.predict(X_test_seq,  verbose=0).squeeze()\n",
        "        # Store actual and predicted returns for later comparison\n",
        "        actual_returns = y_test_seq\n",
        "        predicted_returns = y_pred_test\n",
        "\n",
        "        # Regression metrics\n",
        "        mse  = mean_squared_error(y_test_seq, y_pred_test)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae  = mean_absolute_error(y_test_seq, y_pred_test)\n",
        "        # r2   = r2_score(y_test_seq, y_pred_test)\n",
        "        # Directional metrics\n",
        "        actual_dir = np.sign(y_test_seq[1:] - y_test_seq[:-1])   # actual direction of change\n",
        "        pred_dir   = np.sign(y_pred_test[1:] - y_pred_test[:-1]) # predicted direction\n",
        "        dir_acc = (actual_dir == pred_dir).mean() if len(actual_dir) > 0 else np.nan\n",
        "        actual_sign = (y_test_seq >= 0).astype(int)\n",
        "        pred_sign   = (y_pred_test >= 0).astype(int)\n",
        "        hit_ratio   = (actual_sign == pred_sign).mean()\n",
        "        precision   = precision_score(actual_sign, pred_sign)\n",
        "        recall      = recall_score(actual_sign, pred_sign)\n",
        "        f1          = f1_score(actual_sign, pred_sign)\n",
        "        auc_roc     = roc_auc_score(actual_sign, pred_sign)\n",
        "        cm          = confusion_matrix(actual_sign, pred_sign, labels=[0, 1])\n",
        "        cls_report  = classification_report(actual_sign, pred_sign, target_names=[\"Giảm\",\"Tăng\"], digits=4)\n",
        "        # Print evaluation results\n",
        "        print(f\"=== Evaluation Metrics for {name} ===\")\n",
        "        print(f\"MSE: {mse:.6f},  RMSE: {rmse:.6f},  MAE: {mae:.6f}\")\n",
        "        print(f\"Directional Accuracy: {dir_acc:.4f},  Hit Ratio: {hit_ratio:.4f}\")\n",
        "        print(f\"Precision (Up): {precision:.4f}, Recall (Up): {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "        # print(f\"R²       : {r2:,.6f}\")\n",
        "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "        print(\"Confusion Matrix (0=Down, 1=Up):\\n\", cm)\n",
        "        print(\"Classification Report:\\n\", cls_report)\n",
        "        # Plot actual vs predicted returns if requested\n",
        "        if plot:\n",
        "            plt.figure(figsize=(12,5))\n",
        "            plt.plot(y_test_seq, label=\"Actual Return\", linewidth=2)\n",
        "            plt.plot(y_pred_test, label=\"Predicted Return\", linewidth=2)\n",
        "            plt.title(name)\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(\"Return\")\n",
        "            plt.legend()\n",
        "            plt.grid(alpha=0.3)\n",
        "            plt.show()\n",
        "        # Return metrics in a dictionary for later use\n",
        "        return {\n",
        "            \"mse\": mse, \"rmse\": rmse, \"mae\": mae,\n",
        "            \"directional_accuracy\": dir_acc, \"hit_ratio\": hit_ratio,\n",
        "            \"precision\": precision, \"recall\": recall, \"f1\": f1,\n",
        "            \"auc_roc\": auc_roc,\n",
        "            # \"r2\": r2,\n",
        "            \"actual_returns\": actual_returns, \"predicted_returns\": predicted_returns # Added these lines\n",
        "        }"
      ],
      "metadata": {
        "id": "4L_XSREjwHhM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "8WlH1UX5vXHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Transformer Model (FIXED)\n",
        "# ===============================\n",
        "\n",
        "LOOKBACK = 30\n",
        "\n",
        "def transformer_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"Return\"] = df[\"Log_Return\"]\n",
        "\n",
        "    df[\"RSI\"] = talib.RSI(df[\"Return\"].values, timeperiod=14)\n",
        "    df[\"MACD\"], df[\"MACD_signal\"], _ = talib.MACD(\n",
        "        df[\"Return\"].values, fastperiod=12, slowperiod=26, signalperiod=9\n",
        "    )\n",
        "    df[\"BB_upper\"], df[\"BB_middle\"], df[\"BB_lower\"] = talib.BBANDS(\n",
        "        df[\"Return\"].values, timeperiod=20\n",
        "    )\n",
        "\n",
        "    df[\"EMA_12\"] = df[\"Return\"].ewm(span=12).mean()\n",
        "    df[\"EMA_26\"] = df[\"Return\"].ewm(span=26).mean()\n",
        "    df[\"MA_5\"]   = df[\"Return\"].rolling(5).mean()\n",
        "\n",
        "    df[\"Vol_5\"]  = df[\"Return\"].rolling(5).std()\n",
        "    df[\"Vol_10\"] = df[\"Return\"].rolling(10).std()\n",
        "    df[\"Vol_20\"] = df[\"Return\"].rolling(20).std()\n",
        "\n",
        "    df[\"Trend_20\"] = df[\"Return\"].rolling(20).mean()\n",
        "    df[\"Regime\"]   = (df[\"Vol_20\"] > df[\"Vol_20\"].rolling(60).mean()).astype(int)\n",
        "\n",
        "    return df.dropna()\n",
        "\n",
        "# ---------- Feature Engineering ----------\n",
        "df_trans = transformer_features(data)\n",
        "\n",
        "FEATURES = [\n",
        "    'Return', 'RSI', 'MACD', 'MACD_signal',\n",
        "    'BB_upper', 'BB_middle', 'BB_lower',\n",
        "    'EMA_12', 'EMA_26', 'MA_5',\n",
        "    'Vol_5', 'Vol_10', 'Vol_20',\n",
        "    'Trend_20', 'Regime'\n",
        "]\n",
        "\n",
        "X_raw = df_trans[FEATURES].values\n",
        "y_raw = df_trans[\"Log_Return\"].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_raw)\n",
        "\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_raw, LOOKBACK)\n",
        "\n",
        "split_idx = int(0.8 * len(X_seq))\n",
        "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "# ---------- Positional Encoding ----------\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        pos = np.arange(position)[:, np.newaxis]\n",
        "        i   = np.arange(d_model)[np.newaxis, :]\n",
        "\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "        angle_rads  = pos * angle_rates\n",
        "\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        self.pos_encoding = tf.constant(\n",
        "            angle_rads[np.newaxis, ...], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
        "\n",
        "# ---------- Transformer Encoder ----------\n",
        "def transformer_encoder(x, head_size, num_heads, ff_dim, dropout=0.2):\n",
        "    attn = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=head_size, dropout=dropout\n",
        "    )(x, x)\n",
        "\n",
        "    x = tf.keras.layers.Add()([x, attn])\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    ff = tf.keras.layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = tf.keras.layers.Dropout(dropout)(ff)\n",
        "    ff = tf.keras.layers.Dense(tf.keras.backend.int_shape(x)[-1])(ff)\n",
        "\n",
        "    x = tf.keras.layers.Add()([x, ff])\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# ---------- Build Model ----------\n",
        "def build_transformer(\n",
        "    input_shape,\n",
        "    head_size=64,\n",
        "    num_heads=8,\n",
        "    ff_dim=128,\n",
        "    num_blocks=4,\n",
        "    dropout=0.2\n",
        "):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = PositionalEncoding(input_shape[0], input_shape[1])(inputs)\n",
        "\n",
        "    for _ in range(num_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "    outputs = tf.keras.layers.Dense(1)(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# ---------- Compile ----------\n",
        "model_transformer = build_transformer(\n",
        "    input_shape=(LOOKBACK, X_train.shape[2]),\n",
        "    num_heads=8,\n",
        "    num_blocks=4\n",
        ")\n",
        "\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=20000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "model_transformer.compile(\n",
        "    optimizer=Adam(learning_rate=lr_schedule),\n",
        "    loss=\"mse\"\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_trans = model_transformer.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "U4c6qIrcVE3E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Transformer Model"
      ],
      "metadata": {
        "id": "zoVXLbL3ybh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_trans = evaluate_model(model_transformer, X_train, y_train, X_test, y_test, name=f\"Transformer: {ticker}\", plot=False)\n",
        "metrics_trans[\"Stock\"] = ticker\n",
        "metrics_trans[\"Model\"] = \"Transformer\"\n",
        "all_metrics.append(metrics_trans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYkMAC22wsIg",
        "outputId": "2f6c3a1b-3869-4264-ceee-0ff24a0b5dff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation Metrics for Transformer: MBB.VN ===\n",
            "MSE: 0.000287,  RMSE: 0.016932,  MAE: 0.012569\n",
            "Directional Accuracy: 0.5738,  Hit Ratio: 0.5042\n",
            "Precision (Up): 0.5474, Recall (Up): 0.5725, F1-Score: 0.5597\n",
            "AUC-ROC: 0.4965\n",
            "Confusion Matrix (0=Down, 1=Up):\n",
            " [[45 62]\n",
            " [56 75]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        Giảm     0.4455    0.4206    0.4327       107\n",
            "        Tăng     0.5474    0.5725    0.5597       131\n",
            "\n",
            "    accuracy                         0.5042       238\n",
            "   macro avg     0.4965    0.4965    0.4962       238\n",
            "weighted avg     0.5016    0.5042    0.5026       238\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline LSTM Model"
      ],
      "metadata": {
        "id": "KLRhefupwIbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # === Baseline LSTM Model for this stock ===\n",
        "    def lstm_features_baseline(df):\n",
        "        df = df.copy()\n",
        "        df[\"Return\"] = df[\"Log_Return\"]\n",
        "        # Technical indicators for baseline\n",
        "        df[\"RSI\"] = talib.RSI(df[\"Return\"].values, timeperiod=14)\n",
        "        df[\"MACD\"], df[\"MACD_signal\"], _ = talib.MACD(df[\"Return\"].values, fastperiod=12, slowperiod=26, signalperiod=9)\n",
        "        df[\"BB_upper\"], df[\"BB_middle\"], df[\"BB_lower\"] = talib.BBANDS(df[\"Return\"].values, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
        "        df[\"EMA_12\"] = df[\"Return\"].ewm(span=12, adjust=False).mean()\n",
        "        df[\"EMA_26\"] = df[\"Return\"].ewm(span=26, adjust=False).mean()\n",
        "        df[\"MA_5\"]   = df[\"Return\"].rolling(5).mean()\n",
        "        df[\"Vol_5\"]  = df[\"Return\"].rolling(5).std()\n",
        "        df[\"Vol_10\"] = df[\"Return\"].rolling(10).std()\n",
        "        df['Stochastic_Oscillator'] = talib.STOCH(df['High'], df['Low'], df['Close'], fastk_period=14, slowk_period=3, slowk_matype=0)[0]\n",
        "        df['Average_True_Range'] = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
        "        df['On_Balance_Volume'] = talib.OBV(df['Close'], df['Volume'])\n",
        "        return df.dropna()\n",
        "\n",
        "    df_base = lstm_features_baseline(data)\n",
        "    features_baseline = [\n",
        "    \"Return\", \"EMA_12\", \"EMA_26\", \"Vol_5\", \"Vol_10\", \"MACD\", \"RSI\", \"BB_upper\", \"BB_lower\",\n",
        "    \"Stochastic_Oscillator\", \"Average_True_Range\", \"On_Balance_Volume\"\n",
        "]\n",
        "    X_base = df_base[features_baseline].values\n",
        "    y_base = df_base[\"Return\"].values\n",
        "\n",
        "    # Train-test split (80/20)\n",
        "    split_idx = int(len(X_base) * 0.8)\n",
        "    X_train_base, X_test_base = X_base[:split_idx], X_base[split_idx:]\n",
        "    y_train_base, y_test_base = y_base[:split_idx], y_base[split_idx:]\n",
        "    # Scale features\n",
        "    scaler_base = StandardScaler()\n",
        "    X_train_scaled = scaler_base.fit_transform(X_train_base)\n",
        "    X_test_scaled  = scaler_base.transform(X_test_base)\n",
        "    SEQ_LEN = 30\n",
        "    X_train_seq_base, y_train_seq_base = create_sequences(X_train_scaled, y_train_base, SEQ_LEN)\n",
        "    X_test_seq_base, y_test_seq_base   = create_sequences(X_test_scaled,  y_test_base,  SEQ_LEN)\n",
        "\n",
        "    # Build the baseline LSTM model (bidirectional LSTMs + attention)\n",
        "    from tensorflow.keras.layers import LSTM, Bidirectional, Attention, Dense, Dropout, Input\n",
        "    from tensorflow.keras.models import Model\n",
        "    def build_baseline_model(n_features, seq_len):\n",
        "        inputs = Input(shape=(seq_len, n_features))\n",
        "        # Two Bidirectional LSTM layers\n",
        "        lstm_1   = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
        "        dropout1 = Dropout(0.3)(lstm_1)\n",
        "        lstm_2   = Bidirectional(LSTM(64, return_sequences=True))(dropout1)\n",
        "        dropout2 = Dropout(0.3)(lstm_2)\n",
        "        # Attention mechanism (self-attention on the LSTM outputs)\n",
        "        attention = Attention()([lstm_2, lstm_2])\n",
        "        # Final LSTM to capture long-term dependencies\n",
        "        lstm_3   = LSTM(32)(attention)\n",
        "        dropout3 = Dropout(0.3)(lstm_3)\n",
        "        output   = Dense(1)(dropout3)\n",
        "        model = Model(inputs=inputs, outputs=output)\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        return model\n",
        "\n",
        "    n_features_base = X_train_seq_base.shape[2]\n",
        "    baseline_model = build_baseline_model(n_features_base, SEQ_LEN)\n",
        "    # Early stopping callback\n",
        "    early_stop_base = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "    # Train the LSTM model\n",
        "    history_base = baseline_model.fit(X_train_seq_base, y_train_seq_base,\n",
        "                                      epochs=100, batch_size=32,\n",
        "                                      validation_split=0.2,\n",
        "                                      callbacks=[early_stop_base], verbose=0)"
      ],
      "metadata": {
        "id": "NMDcz5F2wKig"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Baseline LSTM Model\n"
      ],
      "metadata": {
        "id": "m0UREwjFyncv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_base = evaluate_model(baseline_model, X_train_seq_base, y_train_seq_base,\n",
        "                              X_test_seq_base, y_test_seq_base,\n",
        "                              name=f\"Baseline LSTM: {ticker}\", plot=False)\n",
        "metrics_base[\"Stock\"] = ticker\n",
        "metrics_base[\"Model\"] = \"LSTM\"\n",
        "all_metrics.append(metrics_base)"
      ],
      "metadata": {
        "id": "RR9sJ2YsyiXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1301cf9-7de9-4336-e2aa-9f09410382d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation Metrics for Baseline LSTM: MBB.VN ===\n",
            "MSE: 0.000506,  RMSE: 0.022497,  MAE: 0.018562\n",
            "Directional Accuracy: 0.5915,  Hit Ratio: 0.4533\n",
            "Precision (Up): 0.0000, Recall (Up): 0.0000, F1-Score: 0.0000\n",
            "AUC-ROC: 0.5000\n",
            "Confusion Matrix (0=Down, 1=Up):\n",
            " [[ 97   0]\n",
            " [117   0]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        Giảm     0.4533    1.0000    0.6238        97\n",
            "        Tăng     0.0000    0.0000    0.0000       117\n",
            "\n",
            "    accuracy                         0.4533       214\n",
            "   macro avg     0.2266    0.5000    0.3119       214\n",
            "weighted avg     0.2055    0.4533    0.2827       214\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid FFT+LSTM Model"
      ],
      "metadata": {
        "id": "phBO0ulnwP71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fast Fourier Transform"
      ],
      "metadata": {
        "id": "8vfyC7T8wZfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def fft_smooth(series, keep=20):\n",
        "        \"\"\"Keep only the lowest 'keep' frequency components of FFT.\"\"\"\n",
        "        f = np.fft.fft(series)\n",
        "        f[keep:-keep] = 0  # zero out high-frequency components\n",
        "        return np.real(np.fft.ifft(f))"
      ],
      "metadata": {
        "id": "D9kUgtcVwQYf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wavelet Transform"
      ],
      "metadata": {
        "id": "5MTKucmIwfSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def wavelet_transform(series, wavelet='db1', level=3):\n",
        "        coeffs = pywt.wavedec(series, wavelet, level=level)\n",
        "        # Zero out detail coefficients for smoothing\n",
        "        coeffs_smoothed = [coeffs[0]] + [np.zeros_like(c) for c in coeffs[1:]]\n",
        "        smoothed = pywt.waverec(coeffs_smoothed, wavelet)\n",
        "        # Ensure the smoothed series is same length as input\n",
        "        if len(smoothed) > len(series):\n",
        "            smoothed = smoothed[:len(series)]\n",
        "        elif len(smoothed) < len(series):\n",
        "            smoothed = np.pad(smoothed, (0, len(series)-len(smoothed)), mode='edge')\n",
        "        return smoothed"
      ],
      "metadata": {
        "id": "UVeHmJfiwha9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FFT+LSTM"
      ],
      "metadata": {
        "id": "RYXQAD9t5B8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def build_features(df, fft_window_sizes=[10,20,40], wavelet='db1', level=3):\n",
        "        df = df.copy()\n",
        "        df[\"Return\"] = df[\"Log_Return\"]\n",
        "        # Ensure numeric columns are proper type for TA-Lib\n",
        "        for col in [\"Close\",\"High\",\"Low\",\"Volume\",\"Return\"]:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(np.float64)\n",
        "        # FFT-based features\n",
        "        for win in fft_window_sizes:\n",
        "            df[f\"Return_Smooth_{win}\"] = fft_smooth(df[\"Return\"].values, keep=win)\n",
        "            fft_vals = np.fft.fft(df[\"Return\"].values)\n",
        "            fft_abs  = np.abs(fft_vals)\n",
        "            df[f\"FFT_Amplitude_{win}\"] = fft_abs\n",
        "            df[f\"FFT_Phase_{win}\"]     = np.angle(fft_vals)\n",
        "            df[f\"Low_Energy_{win}\"]    = np.sum(fft_abs[:win])\n",
        "            df[f\"High_Energy_{win}\"]   = np.sum(fft_abs[win:])\n",
        "        # Wavelet feature\n",
        "        df[\"Wavelet_Smooth\"] = wavelet_transform(df[\"Return\"].values, wavelet=wavelet, level=level)\n",
        "        # Additional TA indicators\n",
        "        df[\"RSI_14\"] = talib.RSI(df[\"Return\"].values, timeperiod=14)\n",
        "        df[\"RSI_28\"] = talib.RSI(df[\"Return\"].values, timeperiod=28)\n",
        "        df[\"MACD\"], df[\"MACD_signal\"], _ = talib.MACD(df[\"Return\"].values, fastperiod=12, slowperiod=26, signalperiod=9)\n",
        "        df[\"MACD_5\"], df[\"MACD_signal_5\"], _ = talib.MACD(df[\"Return\"].values, fastperiod=5, slowperiod=10, signalperiod=5)\n",
        "        df[\"MACD_20\"], df[\"MACD_signal_20\"], _ = talib.MACD(df[\"Return\"].values, fastperiod=20, slowperiod=40, signalperiod=9)\n",
        "        df[\"BB_upper\"], df[\"BB_middle\"], df[\"BB_lower\"] = talib.BBANDS(df[\"Return\"].values, timeperiod=20, nbdevup=2, nbdevdn=2)\n",
        "        df[\"EMA_12\"] = df[\"Return\"].ewm(span=12, adjust=False).mean()\n",
        "        df[\"EMA_26\"] = df[\"Return\"].ewm(span=26, adjust=False).mean()\n",
        "        df[\"MA_5\"]   = df[\"Return\"].rolling(5).mean()\n",
        "        df[\"Vol_5\"]  = df[\"Return\"].rolling(5).std()\n",
        "        df[\"Vol_10\"] = df[\"Return\"].rolling(10).std()\n",
        "        # On-Balance Volume and Average True Range\n",
        "        df[\"OBV\"]    = talib.OBV(df[\"Close\"].values, df[\"Volume\"].values)\n",
        "        df[\"ATR_14\"] = talib.ATR(df[\"High\"].values, df[\"Low\"].values, df[\"Close\"].values, timeperiod=14)\n",
        "        return df.dropna()\n",
        "\n",
        "    fft_window_sizes = [10, 20, 40]\n",
        "    df_hybrid = build_features(data, fft_window_sizes=fft_window_sizes)\n",
        "    # Define the feature columns for hybrid model\n",
        "    features_hybrid = [\n",
        "        \"Return\", \"EMA_12\", \"EMA_26\", \"Vol_5\", \"Vol_10\",\n",
        "        \"MACD\", \"RSI_14\", \"RSI_28\", \"BB_upper\", \"BB_lower\",\n",
        "        \"OBV\", \"ATR_14\", \"Wavelet_Smooth\", \"MACD_5\", \"MACD_signal_5\",\n",
        "        \"MACD_20\", \"MACD_signal_20\"\n",
        "    ] + [f\"Return_Smooth_{w}\"   for w in fft_window_sizes] \\\n",
        "      + [f\"FFT_Amplitude_{w}\"  for w in fft_window_sizes] \\\n",
        "      + [f\"FFT_Phase_{w}\"      for w in fft_window_sizes] \\\n",
        "      + [f\"Low_Energy_{w}\"     for w in fft_window_sizes] \\\n",
        "      + [f\"High_Energy_{w}\"    for w in fft_window_sizes]\n",
        "\n",
        "    X_hybrid = df_hybrid[features_hybrid].values\n",
        "    y_hybrid = df_hybrid[\"Return\"].values\n",
        "    # Train-test split (80% training data)\n",
        "    split_idx = int(len(X_hybrid) * 0.8)\n",
        "    X_train_h, X_test_h = X_hybrid[:split_idx], X_hybrid[split_idx:]\n",
        "    y_train_h, y_test_h = y_hybrid[:split_idx], y_hybrid[split_idx:]\n",
        "    # Scale features with RobustScaler (to handle outliers in FFT features)\n",
        "    scaler_h = RobustScaler()\n",
        "    X_train_scaled_h = scaler_h.fit_transform(X_train_h)\n",
        "    X_test_scaled_h  = scaler_h.transform(X_test_h)\n",
        "    SEQ_LEN_hybrid = 30\n",
        "    X_train_seq_h, y_train_seq_h = create_sequences(X_train_scaled_h, y_train_h, SEQ_LEN_hybrid)\n",
        "    X_test_seq_h,  y_test_seq_h  = create_sequences(X_test_scaled_h,  y_test_h,  SEQ_LEN_hybrid)\n",
        "\n",
        "    # Build the hybrid LSTM model (same architecture as baseline LSTM)\n",
        "    def build_hybrid_model(n_features, seq_len_hybrid):\n",
        "        inputs = Input(shape=(seq_len_hybrid, n_features))\n",
        "        lstm_1   = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
        "        drop1    = Dropout(0.3)(lstm_1)\n",
        "        lstm_2   = Bidirectional(LSTM(64, return_sequences=True))(drop1)\n",
        "        drop2    = Dropout(0.3)(lstm_2)\n",
        "        attention = Attention()([lstm_2, lstm_2])\n",
        "        lstm_3   = LSTM(32)(attention)\n",
        "        drop3    = Dropout(0.3)(lstm_3)\n",
        "        output   = Dense(1)(drop3)\n",
        "        model    = Model(inputs, output)\n",
        "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        return model\n",
        "    n_features_h = X_train_seq_h.shape[2]\n",
        "    hybrid_model = build_hybrid_model(n_features_h, SEQ_LEN_hybrid)\n",
        "    # Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "    early_stop_h = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "    reduce_lr    = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-6, verbose=0)\n",
        "    checkpoint   = ModelCheckpoint(f\"best_model_{ticker}.h5\", monitor=\"val_loss\", save_best_only=True, mode=\"min\", verbose=0)\n",
        "    # Train the hybrid model\n",
        "    history_hybrid = hybrid_model.fit(X_train_seq_h, y_train_seq_h,\n",
        "                                      epochs=100, batch_size=32,\n",
        "                                      validation_split=0.2,\n",
        "                                      callbacks=[early_stop_h, reduce_lr, checkpoint],\n",
        "                                      verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_fycZnrwmh0",
        "outputId": "9b80000a-b018-422d-9086-4e12b32e4733"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Hybrid FFT+LSTM model"
      ],
      "metadata": {
        "id": "xkUeQkb0yU_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_hybrid = evaluate_model(hybrid_model, X_train_seq_h, y_train_seq_h,\n",
        "                                X_test_seq_h, y_test_seq_h, name=f\"FFT+LSTM: {ticker}\", plot=False)\n",
        "metrics_hybrid[\"Stock\"] = ticker\n",
        "metrics_hybrid[\"Model\"] = \"FFT+LSTM\"\n",
        "all_metrics.append(metrics_hybrid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DYHN3A3yMly",
        "outputId": "8c29fcfa-1221-43fa-b148-6e9640aa79eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Evaluation Metrics for FFT+LSTM: MBB.VN ===\n",
            "MSE: 0.000218,  RMSE: 0.014749,  MAE: 0.011607\n",
            "Directional Accuracy: 0.7714,  Hit Ratio: 0.5782\n",
            "Precision (Up): 0.6195, Recall (Up): 0.6034, F1-Score: 0.6114\n",
            "AUC-ROC: 0.5754\n",
            "Confusion Matrix (0=Down, 1=Up):\n",
            " [[52 43]\n",
            " [46 70]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        Giảm     0.5306    0.5474    0.5389        95\n",
            "        Tăng     0.6195    0.6034    0.6114       116\n",
            "\n",
            "    accuracy                         0.5782       211\n",
            "   macro avg     0.5750    0.5754    0.5751       211\n",
            "weighted avg     0.5795    0.5782    0.5787       211\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison"
      ],
      "metadata": {
        "id": "LobQ0FMxzKFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Container to store actual vs predicted returns for each model and stock\n",
        "all_actual_vs_predicted = []\n",
        "\n",
        "# Loop over each stock and evaluate all models (Transformer, LSTM, FFT+LSTM)\n",
        "# Note: As currently implemented, model_transformer, baseline_model, hybrid_model,\n",
        "# and all X_train/test, y_train/test variables only hold the data/models for the LAST ticker processed\n",
        "# in the preceding cells (MBB.VN). This loop will effectively re-evaluate MBB.VN's models\n",
        "# using MBB.VN's data for each 'ticker' in the list.\n",
        "# A full comparison across all stocks would require storing models and data splits for each stock.\n",
        "for ticker_in_loop in tickers:\n",
        "    # Evaluate Transformer model for current stock (actually last processed stock, MBB.VN)\n",
        "    metrics_trans = evaluate_model(model_transformer, X_train, y_train, X_test, y_test,\n",
        "                                   name=f\"Transformer: {ticker_in_loop}\", plot=False)\n",
        "    all_actual_vs_predicted.append({\n",
        "        \"Stock\": ticker_in_loop,\n",
        "        \"Model\": \"Transformer\",\n",
        "        \"Actual\": metrics_trans[\"actual_returns\"],\n",
        "        \"Predicted\": metrics_trans[\"predicted_returns\"]\n",
        "    })\n",
        "\n",
        "    # Evaluate Baseline LSTM model for current stock (actually last processed stock, MBB.VN)\n",
        "    metrics_lstm = evaluate_model(baseline_model, X_train_seq_base, y_train_seq_base, X_test_seq_base, y_test_seq_base,\n",
        "                                  name=f\"LSTM: {ticker_in_loop}\", plot=False)\n",
        "    all_actual_vs_predicted.append({\n",
        "        \"Stock\": ticker_in_loop,\n",
        "        \"Model\": \"LSTM\",\n",
        "        \"Actual\": metrics_lstm[\"actual_returns\"],\n",
        "        \"Predicted\": metrics_lstm[\"predicted_returns\"]\n",
        "    })\n",
        "\n",
        "    # Evaluate Hybrid FFT+LSTM model for current stock (actually last processed stock, MBB.VN)\n",
        "    metrics_fft_lstm = evaluate_model(hybrid_model, X_train_seq_h, y_train_seq_h, X_test_seq_h, y_test_seq_h,\n",
        "                                      name=f\"FFT+LSTM: {ticker_in_loop}\", plot=False)\n",
        "    all_actual_vs_predicted.append({\n",
        "        \"Stock\": ticker_in_loop,\n",
        "        \"Model\": \"FFT+LSTM\",\n",
        "        \"Actual\": metrics_fft_lstm[\"actual_returns\"],\n",
        "        \"Predicted\": metrics_fft_lstm[\"predicted_returns\"]\n",
        "    })"
      ],
      "metadata": {
        "id": "mxMXwBtCzKjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualization for each model and stock\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Loop over all tickers and plot the actual vs predicted returns\n",
        "for i, ticker in enumerate(tickers, 1):\n",
        "    plt.subplot(3, 2, i)\n",
        "\n",
        "    # Get the results for the current ticker from the all_actual_vs_predicted list\n",
        "    results = [entry for entry in all_actual_vs_predicted if entry[\"Stock\"] == ticker]\n",
        "\n",
        "    for result in results:\n",
        "        model = result[\"Model\"]\n",
        "        actual = result[\"Actual\"]\n",
        "        predicted = result[\"Predicted\"]\n",
        "\n",
        "        # Plot actual vs predicted returns for each model\n",
        "        plt.plot(actual, label=f\"Actual Return ({ticker})\", color=\"yellow\")  # Actual returns in orange\n",
        "        if model == \"LSTM\":\n",
        "            plt.plot(predicted, label=f\"{model} Predicted\", color=\"red\", alpha=0.9)  # LSTM predictions in red\n",
        "        elif model == \"Transformer\":\n",
        "            plt.plot(predicted, label=f\"{model} Predicted\", color=\"blue\", alpha=0.9)  # Transformer predictions in blue\n",
        "        elif model == \"FFT+LSTM\":\n",
        "            plt.plot(predicted, label=f\"{model} Predicted\", color=\"green\", alpha=0.9)  # FFT+LSTM predictions in green\n",
        "\n",
        "    # Customize the plot for each stock\n",
        "    plt.title(f\"{ticker} | Actual vs Predicted Returns\", fontsize=14)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Return\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9UN9TaD52G5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}